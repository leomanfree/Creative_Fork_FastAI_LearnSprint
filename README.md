# 🧠 Creative Fork – Learn Sprint (Fast.ai v22)

This repo documents a 3-day Learn Sprint focused on the Fast.ai 2022 course.

The goal was not just to train a model — but to understand the *minimum viable engine* behind it,  
and to reflect on the philosophical structures that underpin machine learning.

---

## 🏁 Project Summary

Over 72 hours, I:

- Built a pet image classifier using Fast.ai
- Fine-tuned the model with transfer learning
- Rebuilt the architecture from scratch in Colab
- Logged all progress and structured the sprint around the Build → Distill → Share loop
- Integrated reflections on classical logic, dialectical thinking, and hybrid human-AI cognition

---

## 📁 Notebooks

- [Day 1 – Build](link-to-day1-notebook)
- [Day 2 – Distill](link-to-day2-notebook)
- [Day 3 – Refine & Reflect](link-to-day3-notebook)

---

## 🔍 Key Concepts

- Minimum Viable Engine: `DataBlock → Learner → Train → Predict`
- Transfer Learning & `learn.unfreeze()`
- Layer structures and abstraction
- Evaluation via prediction (`learn.predict(img)`)

---

## 🧠 Philosophy Blocks

This Learn Sprint also explores foundational ideas behind modern AI:

---

### 1. Aristotelian Structure

> "Each layer performs a logical abstraction — from perception to reason."

Layers as **classes of form** (Aristotle, *Posterior Analytics*)  
ML as a continuation of syllogistic reasoning in computational form.

📚 Ref: Lukasiewicz, *Aristotle's Syllogistic from the Standpoint of Modern Formal Logic*

---

### 2. Hegelian Dialectic & Logical Form

> "Knowledge arises not from repetition, but from contradiction."

Inspired by Hegel’s *Science of Logic*:

- The logic of becoming  
- Contradiction as the motor of development  
- Abstraction through negation  
- Categories as dynamic, not static

Just like in backpropagation:  
→ The model learns by negating error, not just reinforcing signal.

📚 Ref: Hegel, *Science of Logic* (1812–16)

---

### 3. Beyond Statistics: Hybrid Intelligence

> "LLMs are brilliant sophists. They optimize for likelihood, not meaning."

Reflections on von Franz, Jung, and the limits of probabilistic inference.  
The future isn’t artificial minds — it’s **engineered human-AI cognition.**

From co-pilots to **cognitive stacks**.  
From fine-tuning weights to fine-tuning minds.

📚 Ref: von Franz, *On Divination and Synchronicity*  
📄 Ref: *Self-Reflecting LLMs: A Hegelian Dialectical Approach* (arXiv:2501.14917)

---

## 🚀 Closing Thoughts

This sprint was a testbed.  
The repo is live, the model runs, but the real experiment is:

> How far can we stretch the meaning of “learning” — with and beyond AI?

DM me if you're exploring the same questions.
