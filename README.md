# ğŸ§  Creative Fork â€“ Learn Sprint (Fast.ai v22)

This repo documents a 3-day Learn Sprint focused on the Fast.ai 2022 course.

The goal was not just to train a model â€” but to understand the *minimum viable engine* behind it,  
and to reflect on the philosophical structures that underpin machine learning.

---

## ğŸ Project Summary

Over 72 hours, I:

- Built a pet image classifier using Fast.ai
- Fine-tuned the model with transfer learning
- Rebuilt the architecture from scratch in Colab
- Logged all progress and structured the sprint around the Build â†’ Distill â†’ Share loop
- Integrated reflections on classical logic, dialectical thinking, and hybrid human-AI cognition

---

## ğŸ§­ [Learn Log] (https://deserted-ladybug-896.notion.site/Learn-Log-1c7e55b865378010982dff575412b8f2?pvs=4)

---

## ğŸ“ Notebooks

- [Day 1 â€“ Build](Creative_Fork_FastAI_LearnSprint.ipynb)
- [Day 2 â€“ Distill](02_Model_Rebuild_From_Memory.ipynb)
- [Day 3 â€“ Refine & Reflect](03_Model_Rebuild_From_Memory.ipynb)

---

## ğŸ” Key Concepts

- Minimum Viable Engine: `DataBlock â†’ Learner â†’ Train â†’ Predict`
- Transfer Learning & `learn.unfreeze()`
- Layer structures and abstraction
- Evaluation via prediction (`learn.predict(img)`)

---

## ğŸ§  Philosophy Blocks

This Learn Sprint also explores foundational ideas behind modern AI:

---

### 1. Aristotelian Structure

> "Each layer performs a logical abstraction â€” from perception to reason."

Layers as **classes of form** (Aristotle, *Posterior Analytics*)  
ML as a continuation of syllogistic reasoning in computational form.

ğŸ“š Ref: Lukasiewicz, *Aristotle's Syllogistic from the Standpoint of Modern Formal Logic*

---

### 2. Hegelian Dialectic & Logical Form

> "Knowledge arises not from repetition, but from contradiction."

Inspired by Hegelâ€™s *Science of Logic*:

- The logic of becoming  
- Contradiction as the motor of development  
- Abstraction through negation  
- Categories as dynamic, not static

Just like in backpropagation:  
â†’ The model learns by negating error, not just reinforcing signal.

ğŸ“š Ref: Hegel, *Science of Logic* (1812â€“16)

---

### 3. Beyond Statistics: Hybrid Intelligence

> "LLMs are brilliant sophists. They optimize for likelihood, not meaning."

Reflections on von Franz, Jung, and the limits of probabilistic inference.  
The future isnâ€™t artificial minds â€” itâ€™s **engineered human-AI cognition.**

From co-pilots to **cognitive stacks**.  
From fine-tuning weights to fine-tuning minds.

ğŸ“š Ref: von Franz, *On Divination and Synchronicity*  
ğŸ“„ Ref: *Self-Reflecting LLMs: A Hegelian Dialectical Approach* (arXiv:2501.14917)

---

## ğŸš€ Closing Thoughts

This sprint was a testbed.  
The repo is live, the model runs, but the real experiment is:

> How far can we stretch the meaning of â€œlearningâ€ â€” with and beyond AI?

DM me if you're exploring the same questions.

ğŸ”® Alchemical Symbols Classifier â€“ Bonus Remix

As part of the FastAI Learnathon @ Network School, I challenged myself to ship a bonus model â€” weird, symbolic, and fast.

**A classifier of alchemical glyphs.**  
Trained on a small handcrafted dataset representing key archetypal elements and classical substances.

ğŸ“‚ Dataset  
Originally, I tried to render Unicode alchemical symbols using Python + PIL â€” but the Unicode-to-font rendering was **pretty buggy** (some glyphs wouldnâ€™t display or save properly).

So I pivoted:  
â†’ I manually built a tiny dataset with simple clean images representing:

- ğŸŒ¬ Air  
- ğŸ”¥ Fire  
- ğŸ’§ Water  
- ğŸŒ Earth  
- ğŸœš Gold  
- ğŸœ› Silver  
- ğŸœ” Salt  
- ğŸœ Sulfur  
- â˜¿ Mercury

ğŸ§  Goal  
Test the FastAI pipeline on abstract/symbolic concepts.  
Play with minimal data. Ship something weird. Earn ğŸ•.

ğŸ› ï¸ Tech Stack

- Custom image dataset (hand-curated)
- Fast.ai `DataBlock` and `cnn_learner`
- Trained with ResNet-18 and `learn.fine_tune(4)`
- Ran predictions on single-symbol test images

ğŸ“ˆ Results  
Even with very limited data, model reached solid accuracy due to class clarity.  
The real win: remixing the pipeline, debugging hard font issues, and shipping.

ğŸ“ Notebook  
See: `day3_alchemy_classifier.ipynb`

---

ğŸ™ Thanks to the Learnathon for the permission to explore.

ğŸ§˜â€â™‚ï¸ *â€œYour notebook is your temple. Your output is your offering.â€*

#fastai #learnathon #symbolicML #alchemy #creativefork
